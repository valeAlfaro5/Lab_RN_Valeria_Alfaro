{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0b6e89-b8bd-4b59-b762-7f11aa45ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import DnnLib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0534c621-5da3-46ba-a27e-d73620de40df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.213275\n",
      "Epoch 20, Loss: 0.187392\n",
      "Epoch 40, Loss: 0.172109\n",
      "Epoch 60, Loss: 0.168018\n",
      "Epoch 80, Loss: 0.167389\n"
     ]
    }
   ],
   "source": [
    "#Example 1\n",
    "\n",
    "# Create sample data (XOR problem)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float64)\n",
    "y = np.array([[0], [1], [1], [0]], dtype=np.float64)\n",
    "\n",
    "\n",
    "# Create a neural network: 2 -> 4 -> 1\n",
    "layer1 = DnnLib.DenseLayer(2, 4, DnnLib.ActivationType.RELU)\n",
    "layer2 = DnnLib.DenseLayer(4, 1, DnnLib.ActivationType.SIGMOID)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = DnnLib.Adam(learning_rate=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    h1 = layer1.forward(X)\n",
    "    output = layer2.forward(h1)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = DnnLib.mse(output, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_grad = DnnLib.mse_gradient(output, y)\n",
    "    grad2 = layer2.backward(loss_grad)\n",
    "    grad1 = layer1.backward(grad2)\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.update(layer2)\n",
    "    optimizer.update(layer1)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9fbb5a-23b5-4607-a118-5ad424e6e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dense Layers\n",
    "\n",
    "# Basic dense layer: input_dim=3, output_dim=5, default ReLU activation\n",
    "layer = DnnLib.DenseLayer(3, 5)\n",
    "\n",
    "# With specific activation function\n",
    "layer = DnnLib.DenseLayer(3, 5, DnnLib.ActivationType.SIGMOID)\n",
    "\n",
    "#operations\n",
    "single_input = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "batch_input = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=np.float64)\n",
    "\n",
    "output = layer.forward(single_input) # Shape: (5,)\n",
    "batch_output = layer.forward(batch_input) # Shape: (2, 5)\n",
    "\n",
    "# Forward pass without activation (linear only)\n",
    "linear_output = layer.forward_linear(single_input)\n",
    "\n",
    "# Backward pass (for gradient computation)\n",
    "gradient_input = np.ones(5, dtype=np.float64) # Gradient from next layer\n",
    "input_gradient = layer.backward(gradient_input)\n",
    "\n",
    "# Access parameters\n",
    "weights = layer.weights # Shape: (output_dim, input_dim)\n",
    "bias = layer.bias # Shape: (output_dim,)\n",
    "\n",
    "# Access gradients (after backward pass)\n",
    "weight_grads = layer.weight_gradients\n",
    "bias_grads = layer.bias_gradients\n",
    "\n",
    "# Modify activation function\n",
    "layer.activation_type = DnnLib.ActivationType.TANH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "279a15c3-9563-4177-a2d3-c60d4933acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000 samples, 2 features\n",
      "Positive class: 514.0/1000 samples\n",
      "Training binary classifier...\n",
      "Epoch 0, Loss: 0.9124, Accuracy: 0.4930\n",
      "Epoch 40, Loss: 0.1307, Accuracy: 0.9870\n",
      "Epoch 80, Loss: 0.0372, Accuracy: 0.9980\n",
      "Epoch 120, Loss: 0.0269, Accuracy: 0.9970\n",
      "Epoch 160, Loss: 0.0225, Accuracy: 0.9980\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "#binary classification\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(1000, 2).astype(np.float64)\n",
    "y = (x[:,0] + x[:, 1] > 0).astype(np.float64).reshape(-1,1)\n",
    "\n",
    "print(f\"Dataset: {x.shape[0]} samples, {x.shape[1]} features\")\n",
    "print(f\"Positive class: {np.sum(y)}/{len(y)} samples\")\n",
    "\n",
    "layers = [\n",
    " DnnLib.DenseLayer(2, 8, DnnLib.ActivationType.RELU),\n",
    " DnnLib.DenseLayer(8, 4, DnnLib.ActivationType.RELU),\n",
    " DnnLib.DenseLayer(4, 1, DnnLib.ActivationType.SIGMOID)\n",
    "]\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = DnnLib.Adam(learning_rate=0.01)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training binary classifier...\")\n",
    "for epoch in range(200):\n",
    "    # Forward pass\n",
    "    h1 = layers[0].forward(x)\n",
    "    h2 = layers[1].forward(h1)\n",
    "    output = layers[2].forward(h2)\n",
    "    \n",
    "    # Binary cross-entropy loss\n",
    "    loss = DnnLib.binary_cross_entropy(output, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad = DnnLib.binary_cross_entropy_gradient(output, y)\n",
    "    grad = layers[2].backward(grad)\n",
    "    grad = layers[1].backward(grad)\n",
    "    grad = layers[0].backward(grad)\n",
    "    \n",
    "    # Update all layers\n",
    "    optimizer.update(layers[2])\n",
    "    optimizer.update(layers[1])\n",
    "    optimizer.update(layers[0])\n",
    "    \n",
    "    if epoch % 40 == 0:\n",
    "        # Calculate accuracy\n",
    "        predictions = (output > 0.5).astype(np.float64)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd7362d-ee20-456c-a663-c414a7ed46d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-class dataset: (1500, 4), 3 classes\n",
      "Training multi-class classifier...\n",
      "Epoch 0, Loss: 8.9168, Accuracy: 0.3307\n",
      "Epoch 30, Loss: 5.5706, Accuracy: 0.3453\n",
      "Epoch 60, Loss: 7.2441, Accuracy: 0.3460\n",
      "Epoch 90, Loss: 10.5551, Accuracy: 0.3453\n",
      "Epoch 120, Loss: 14.0342, Accuracy: 0.3333\n",
      "Multi-class training completed!\n"
     ]
    }
   ],
   "source": [
    "#Example 2\n",
    "# Generate 3-class classification data\n",
    "np.random.seed(42)\n",
    "n_samples = 1500\n",
    "n_features = 4\n",
    "n_classes = 3\n",
    "\n",
    "X = np.random.randn(n_samples, n_features).astype(np.float64)\n",
    "\n",
    "# Create 3 classes with different patterns\n",
    "y_labels = np.zeros(n_samples, dtype=int)\n",
    "y_labels[:500] = 0 # Class 0\n",
    "y_labels[500:1000] = 1 # Class 1\n",
    "y_labels[1000:] = 2 # Class 2\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y = np.zeros((n_samples, n_classes), dtype=np.float64)\n",
    "y[np.arange(n_samples), y_labels] = 1.0\n",
    "\n",
    "print(f\"Multi-class dataset: {X.shape}, {n_classes} classes\")\n",
    "\n",
    "# Create network: 4 -> 16 -> 8 -> 3\n",
    "layers = [\n",
    "DnnLib.DenseLayer(4, 16, DnnLib.ActivationType.RELU),\n",
    "DnnLib.DenseLayer(16, 8, DnnLib.ActivationType.RELU),\n",
    "DnnLib.DenseLayer(8, 3, DnnLib.ActivationType.SOFTMAX)\n",
    "]\n",
    "\n",
    "# Use RMSprop optimizer\n",
    "optimizer = DnnLib.RMSprop(learning_rate=0.01, decay_rate=0.9)\n",
    "print(\"Training multi-class classifier...\")\n",
    "\n",
    "for epoch in range(150):\n",
    "# Forward pass\n",
    "    h1 = layers[0].forward(X)\n",
    "    h2 = layers[1].forward(h1)\n",
    "    output = layers[2].forward(h2)\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    loss = DnnLib.cross_entropy(output, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad = DnnLib.cross_entropy_gradient(output, y)\n",
    "    grad = layers[2].backward(grad)\n",
    "    grad = layers[1].backward(grad)\n",
    "    grad = layers[0].backward(grad)\n",
    "    \n",
    "    # Update parameters\n",
    "    for layer in layers:\n",
    "        optimizer.update(layer)\n",
    "        \n",
    "    if epoch % 30 == 0:\n",
    "        # Calculate accuracy\n",
    "        predicted_classes = np.argmax(output, axis=1)\n",
    "        accuracy = np.mean(predicted_classes == y_labels)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "print(\"Multi-class training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7887553-3120-4f56-86d5-ed977dc0ecd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression dataset: (2000, 2) -> (2000, 1)\n",
      "Target range: [-0.19, 7.95]\n",
      "\n",
      "--- Training with SGD ---\n",
      " Epoch 0, Loss: 10.165082, R²: -2.5072\n",
      " Epoch 25, Loss: 10.165082, R²: -2.5072\n",
      " Epoch 50, Loss: 10.165082, R²: -2.5072\n",
      " Epoch 75, Loss: 10.165082, R²: -2.5072\n",
      " Final SGD Loss: 10.165082\n",
      "\n",
      "--- Training with SGD+Momentum ---\n",
      " Epoch 0, Loss: 10.011617, R²: -2.4542\n",
      " Epoch 25, Loss: 7.035337, R²: -1.4274\n",
      " Epoch 50, Loss: 0.571781, R²: 0.8027\n",
      " Epoch 75, Loss: 0.249464, R²: 0.9139\n",
      " Final SGD+Momentum Loss: 0.175653\n",
      "\n",
      "--- Training with Adam ---\n",
      " Epoch 0, Loss: 165.291529, R²: -56.0296\n",
      " Epoch 25, Loss: 25.489426, R²: -7.7945\n",
      " Epoch 50, Loss: 7.915615, R²: -1.7311\n",
      " Epoch 75, Loss: 6.809272, R²: -1.3494\n",
      " Final Adam Loss: 6.448034\n",
      "\n",
      "--- Training with RMSprop ---\n",
      " Epoch 0, Loss: 419.796687, R²: -143.8400\n",
      " Epoch 25, Loss: 74.152471, R²: -24.5844\n",
      " Epoch 50, Loss: 19.237093, R²: -5.6373\n",
      " Epoch 75, Loss: 8.708759, R²: -2.0047\n",
      " Final RMSprop Loss: 6.031800\n"
     ]
    }
   ],
   "source": [
    "#example 3 - regression\n",
    "# Generate regression data: y = x₁² + x₂² + noise\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "X = np.random.uniform(-2, 2, size=(n_samples, 2)).astype(np.float64)\n",
    "y = (X[:, 0]**2 + X[:, 1]**2 + 0.1 * np.random.randn(n_samples)).reshape(-1, 1)\n",
    "\n",
    "print(f\"Regression dataset: {X.shape} -> {y.shape}\")\n",
    "print(f\"Target range: [{np.min(y):.2f}, {np.max(y):.2f}]\")\n",
    "\n",
    "# Create deeper network for regression: 2 -> 32 -> 16 -> 8 -> 1\n",
    "layers = [\n",
    "    DnnLib.DenseLayer(2, 32, DnnLib.ActivationType.RELU),\n",
    "    DnnLib.DenseLayer(32, 16, DnnLib.ActivationType.RELU),\n",
    "    DnnLib.DenseLayer(16, 8, DnnLib.ActivationType.RELU),\n",
    "    DnnLib.DenseLayer(8, 1, DnnLib.ActivationType.RELU) \n",
    "    # No activation for regression output\n",
    "]\n",
    "\n",
    "# Try different optimizers\n",
    "optimizers = [\n",
    "(\"SGD\", DnnLib.SGD(0.001)),\n",
    "(\"SGD+Momentum\", DnnLib.SGD(0.001, 0.9)),\n",
    "(\"Adam\", DnnLib.Adam(0.001)),\n",
    "(\"RMSprop\", DnnLib.RMSprop(0.001))\n",
    "]\n",
    "\n",
    "for opt_name, optimizer in optimizers:\n",
    "    print(f\"\\n--- Training with {opt_name} ---\")\n",
    "    # Reset network weights (create new layers)\n",
    "    layers = [\n",
    "        DnnLib.DenseLayer(2, 32, DnnLib.ActivationType.RELU),\n",
    "        DnnLib.DenseLayer(32, 16, DnnLib.ActivationType.RELU),\n",
    "        DnnLib.DenseLayer(16, 8, DnnLib.ActivationType.RELU),\n",
    "        DnnLib.DenseLayer(8, 1, DnnLib.ActivationType.RELU)\n",
    "    ]\n",
    "    optimizer.reset()\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(100):\n",
    "        # Forward pass\n",
    "        activation = X\n",
    "        \n",
    "        for layer in layers:\n",
    "            activation = layer.forward(activation)\n",
    "        output = activation\n",
    "        \n",
    "        # MSE loss for regression\n",
    "        loss = DnnLib.mse(output, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad = DnnLib.mse_gradient(output, y)\n",
    "        for layer in reversed(layers):\n",
    "            grad = layer.backward(grad)\n",
    "            optimizer.update(layer)\n",
    "        if epoch % 25 == 0:\n",
    "        # Calculate R² score\n",
    "            ss_res = np.sum((y - output) ** 2)\n",
    "            ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "            r2_score = 1 - (ss_res / ss_tot)\n",
    "            print(f\" Epoch {epoch}, Loss: {loss:.6f}, R²: {r2_score:.4f}\")\n",
    "    print(f\" Final {opt_name} Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb965e3f-e751-40cf-99d7-dbc76931ced4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
