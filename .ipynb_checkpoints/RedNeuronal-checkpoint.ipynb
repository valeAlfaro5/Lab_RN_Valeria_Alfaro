{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa11ba5c-c01c-43d3-86e8-19a7b7400afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import DnnLib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02a3552-4536-4d93-9a79-98c64ca89055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga de datos\n",
    "with open(\"mnist_mlp_pretty.json\", \"r\") as f:\n",
    "    datos = json.load(f)\n",
    "\n",
    "#carga imagenes y labels\n",
    "data = np.load(\"mnist_test.npz\")\n",
    "images = data[\"images\"]\n",
    "labels = data[\"labels\"]\n",
    "\n",
    "pruebas = np.load(\"mnist_train.npz\")\n",
    "image = pruebas[\"images\"]\n",
    "label = pruebas[\"labels\"]\n",
    "\n",
    "#capa1\n",
    "w1 = np.array(datos[\"layers\"][0][\"W\"])\n",
    "b1 = np.array(datos[\"layers\"][0][\"b\"])\n",
    "\n",
    "#capa2\n",
    "w2 = np.array(datos[\"layers\"][1][\"W\"])\n",
    "b2 = np.array(datos[\"layers\"][1][\"b\"])\n",
    "\n",
    "#activación por capa\n",
    "activate1 = datos[\"layers\"][0][\"activation\"]\n",
    "activate2 = datos[\"layers\"][1][\"activation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d02c5d4-95ac-4118-bb77-0e8c5ab0b8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 128) (128,) (128, 10) (10,) (60000,) (60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(w1.shape, b1.shape, w2.shape, b2.shape, label.shape, image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b23274-7c71-418c-b5d3-d73db638e5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 784)\n"
     ]
    }
   ],
   "source": [
    "#definir capas densas (ambas son densas segun el json) - parte 3\n",
    "#primera capa tiene 784 entradas y 128 salidas, activacion relu\n",
    "layer1 = DnnLib.DenseLayer(784, 128, DnnLib.ActivationType.RELU)\n",
    "#segunda capa tiene 128 entradas(de capa 1) y 10 de salida, activacion softmax\n",
    "layer2 = DnnLib.DenseLayer(128, 10, DnnLib.ActivationType.SOFTMAX)\n",
    "\n",
    "#definir pesos y biases tomados del archivo\n",
    "#transponer para evitar errores\n",
    "layer1.weights = w1.T\n",
    "layer1.bias = b1.T\n",
    "layer2.weights = w2.T\n",
    "layer2.bias = b2.T\n",
    "\n",
    "x = np.random.rand(1, 784)\n",
    "print(w1.T.shape)\n",
    "\n",
    "output = layer1.forward(x)\n",
    "salida = layer2.forward(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc25efe-5742-44d2-a4da-2f883223dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida capa 1: (1, 128)\n",
      "Salida capa 2: (1, 10)\n",
      "Predicción: 0.0\n"
     ]
    }
   ],
   "source": [
    "#predicciones de prueba\n",
    "predict = np.argmax(salida, axis =1)\n",
    "#acurracy\n",
    "acurracy = np.mean(predict == x)\n",
    "\n",
    "#verificar que funcione la salidas de cada una con su predicción\n",
    "print(\"Salida capa 1:\", output.shape)\n",
    "print(\"Salida capa 2:\", salida.shape)\n",
    "print(\"Predicción:\", acurracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9baee6-e45a-4411-8a9e-4de782b3e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entrenamiento de red neuronal - parte 4\n",
    "#creando arreglos & variables necesarias\n",
    "\n",
    "capa1 = DnnLib.DenseLayer(784, 128, DnnLib.ActivationType.RELU)\n",
    "\n",
    "#segunda capa tiene 128 entradas(de capa 1) y 10 de salida, activacion softmax\n",
    "capa2 = DnnLib.DenseLayer(128, 10, DnnLib.ActivationType.SOFTMAX)\n",
    "\n",
    "capa = [capa1, capa2]\n",
    "\n",
    "#arreglo de optimizadores\n",
    "optimizers = DnnLib.Adam(0.001)\n",
    "\n",
    "pruebas = np.load(\"mnist_train.npz\")\n",
    "image = pruebas[\"images\"]\n",
    "label = pruebas[\"labels\"]\n",
    "\n",
    "f = image.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#creación de one hot\n",
    "n = label.shape[0]  \n",
    "y = np.zeros((n, 10), dtype=np.float64)\n",
    "y[np.arange(n), label] = 1.0\n",
    "\n",
    "\n",
    "#arreglo de layers ya definidio, arreglo de todos los optimizadores, f son las entradas del mnist-train, y es el one hot, label a lo que estoy comparando y epochs epocas predefinidas, batch sizes para que pueda correr sin tronar \n",
    "def entrenamiento1(capas, optimizer, f, y_onehot, label, epochs, batch_size=128):\n",
    "    n = f.shape[0]\n",
    "    for e in range(epochs):\n",
    "        indices = np.random.permutation(n)\n",
    "        print(\"Barajado correcto 1.\")\n",
    "        f, y_onehot, label = f[indices], y_onehot[indices], label[indices]\n",
    "        print(\"Barajado correcto 2.\")\n",
    "\n",
    "        # datos para calcular promedio de loss y accuracy\n",
    "        loss_epoca = 0.0\n",
    "        nb, correct, total = 0, 0, 0\n",
    "\n",
    "        for i in range(0, n, batch_size):\n",
    "            X_batch = f[i:i+batch_size]\n",
    "            y_batch = y_onehot[i:i+batch_size]\n",
    "            label_batch = label[i:i+batch_size]\n",
    "\n",
    "            # forward\n",
    "            out1 = capas[0].forward(X_batch)\n",
    "            out2 = capas[1].forward(out1)\n",
    "\n",
    "            # loss\n",
    "            perdida = DnnLib.cross_entropy(out2, y_batch)\n",
    "\n",
    "            # backward\n",
    "            grad = DnnLib.cross_entropy_gradient(out2, y_batch)\n",
    "            grad = capas[1].backward(grad)\n",
    "            grad = capas[0].backward(grad)\n",
    "\n",
    "            # actualización con Adam\n",
    "            optimizer.update(capas[1])\n",
    "            optimizer.update(capas[0])\n",
    "\n",
    "            # acumular métricas\n",
    "            loss_epoca += perdida\n",
    "            nb += 1\n",
    "\n",
    "            preds = np.argmax(out2, axis=1)\n",
    "            correct += np.sum(preds == label_batch)\n",
    "            total += len(label_batch)\n",
    "            \n",
    "            # if i % 50 == 0:  # cada 50 batches\n",
    "            #     print(f\"Batch {i}, Loss: {perdida:.4f}, Accuracy: {correct}\")\n",
    "\n",
    "        # promedios por época\n",
    "        avg_loss = loss_epoca / nb\n",
    "        acc = correct / total\n",
    "\n",
    "        print(f\"Epoch {e+1}/{epochs}, Loss: {avg_loss:.4f}, Acc: {acc*100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27a1db4-b92a-4361-b09d-998bf6a1250b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 1/10, Loss: 0.3787, Acc: 89.86%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 2/10, Loss: 0.1746, Acc: 94.96%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 3/10, Loss: 0.1260, Acc: 96.29%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 4/10, Loss: 0.0974, Acc: 97.15%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 5/10, Loss: 0.0794, Acc: 97.75%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 6/10, Loss: 0.0655, Acc: 98.04%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 7/10, Loss: 0.0555, Acc: 98.38%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 8/10, Loss: 0.0471, Acc: 98.64%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 9/10, Loss: 0.0390, Acc: 98.90%\n",
      "Barajado correcto 1.\n",
      "Barajado correcto 2.\n",
      "Epoch 10/10, Loss: 0.0339, Acc: 99.07%\n"
     ]
    }
   ],
   "source": [
    "#llamada de función de entrenamiento\n",
    "print(\"Mnist Train\")\n",
    "entrenamiento1(capa, optimizers, f, y, label, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c2630f-1e4f-43ec-a828-15c42fab560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar(capas):\n",
    "    modelo = {\n",
    "        \"input_shape\": [28, 28],\n",
    "        \"preprocess\": {\n",
    "            \"scale\": 255\n",
    "        },\n",
    "        \"layers\": []\n",
    "    }\n",
    "    layer1= {\n",
    "        \"type\": \"dense\",\n",
    "        \"units\": 128,\n",
    "        \"activation\": \"relu\",\n",
    "        \"W\": capas[0].weights.tolist(),\n",
    "        \"b\": capas[0].bias.tolist()\n",
    "    }\n",
    "    layer2 = {\n",
    "        \"type\": \"dense\",\n",
    "        \"units\": 10,\n",
    "        \"activation\": \"softmax\",\n",
    "        \"W\": capas[1].weights.tolist(),\n",
    "        \"b\": capas[1].bias.tolist()\n",
    "    }\n",
    "    \n",
    "\n",
    "    modelo[\"layers\"].append(layer1)\n",
    "    modelo[\"layers\"].append(layer2)\n",
    "    \n",
    "    with open(\"mnist_entrenado.json\", \"w\") as f:\n",
    "        json.dump(modelo, f)\n",
    "\n",
    "cargar(capa)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66380c3-8652-4e09-9ffb-42fccd87a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Entrenamiento de red neuronal - Valeria\")\n",
    "\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50,\n",
    "                        help=\"Número de épocas para entrenar\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128,\n",
    "                        help=\"Tamaño de cada batch\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001,\n",
    "                        help=\"Learning rate para el optimizador\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    \n",
    "    capa1 = DnnLib.DenseLayer(784, 128, DnnLib.ActivationType.RELU)\n",
    "    capa2 = DnnLib.DenseLayer(128, 10, DnnLib.ActivationType.SOFTMAX)\n",
    "    capa = [capa1, capa2]\n",
    "\n",
    "    optimizers = DnnLib.Adam(args.lr)\n",
    "\n",
    "    pruebas = np.load(args.train_file)\n",
    "    image = pruebas[\"images\"]\n",
    "    label = pruebas[\"labels\"]\n",
    "\n",
    "    f = image.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "    # One hot\n",
    "    n = label.shape[0]\n",
    "    y = np.zeros((n, 10), dtype=np.float64)\n",
    "    y[np.arange(n), label] = 1.0\n",
    "\n",
    "    entrenamiento1(capa, optimizers, f, y, label, epochs=args.epochs, batch_size=args.batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
