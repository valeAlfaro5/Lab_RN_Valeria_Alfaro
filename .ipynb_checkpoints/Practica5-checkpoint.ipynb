{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc0f36a-efa5-470e-b83f-144038391699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import DnnLib\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "\n",
    "# carga datos de npz para labels e imagenes\n",
    "prueba = np.load(\"fashion_mnist_train.npz\")\n",
    "test = np.load(\"fashion_mnist_test.npz\")\n",
    "\n",
    "#separar en labels e imagenes\n",
    "imagenes = prueba[\"images\"]\n",
    "labelE = prueba[\"labels\"]\n",
    "\n",
    "imagens = test[\"images\"]\n",
    "labelP = test[\"labels\"]\n",
    "\n",
    "#reformar las imagenes(entradas)\n",
    "imagenE = imagenes.reshape(-1, 784).astype(np.float32) / 255\n",
    "imagenP = imagens.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "# funcion de one hot\n",
    "def to_one_hot(labels, num_classes=10):\n",
    "    h = np.zeros((labels.shape[0], num_classes), dtype=np.float32)\n",
    "    h[np.arange(labels.shape[0]), labels] = 1\n",
    "    return h\n",
    "\n",
    "#one hot para cada labels\n",
    "yE = to_one_hot(labelE)\n",
    "yP = to_one_hot(labelP)\n",
    "\n",
    "# cargar json\n",
    "def load_datos_reg():\n",
    "    with open(\"mnist_entrenado.json\", \"r\") as f:\n",
    "        datos = json.load(f)\n",
    "\n",
    "    # Capa 1 \n",
    "    layer1 = DnnLib.DenseLayer(784, 128, DnnLib.ActivationType.RELU)\n",
    "    \n",
    "    # Dropout\n",
    "    dropout1 = DnnLib.Dropout(dropout_rate=0.5)\n",
    "\n",
    "    # Capa 2\n",
    "    layer2 = DnnLib.DenseLayer(128, 10, DnnLib.ActivationType.SOFTMAX)\n",
    "    \n",
    "    print(\"pesos\")\n",
    "    return [layer1, dropout1, layer2]\n",
    "\n",
    "#funciones de forward y backward para droupout\n",
    "def forward_pass_with_dropout(layers, x, training=True):\n",
    "    activations = [x] \n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'training'):\n",
    "            layer.training = training\n",
    "        activations.append(layer.forward(activations[-1]))\n",
    "    return activations\n",
    "\n",
    "def backward_pass_with_dropout(layers, grad_output):\n",
    "    grad = grad_output\n",
    "    for layer in reversed(layers):\n",
    "        grad = layer.backward(grad)\n",
    "    return grad\n",
    "\n",
    "def entrenamiento_reg(capas, optimizers, x, y, label, X_val, y_val, epochs=50, batches=128):\n",
    "    n = x.shape[0]\n",
    "    for e in range(1, epochs+1):\n",
    "        r = np.random.permutation(n)\n",
    "        # print(\"Bajado correcto 1\")\n",
    "        x_shuffled = x[r]\n",
    "        y_shuffled = y[r]\n",
    "        # print(\"Bajado correcto 2\")\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        n_batches, correct, total = 0, 0, 0\n",
    "\n",
    "        for i in range(0, n, batches):\n",
    "            x_batch = x_shuffled[i:i+batches]\n",
    "            y_batch = y_shuffled[i:i+batches]\n",
    "            # print(\"Bajado correcto 3\")\n",
    "            \n",
    "            # forward y dropout \n",
    "            output = forward_pass_with_dropout(capas, x_batch, training=True)\n",
    "            \n",
    "            penultima = output[-2]\n",
    "            out_linear = capas[-1].forward_linear(penultima)\n",
    "\n",
    "            # perdida\n",
    "            perdida = DnnLib.cross_entropy(out_linear, y_batch)\n",
    "\n",
    "            # regularizar\n",
    "            total_reg_loss =0.0\n",
    "            total_reg_loss = capas[0].compute_regularization_loss() + capas[2].compute_regularization_loss()\n",
    "            data_loss = perdida + total_reg_loss\n",
    "\n",
    "            # backward con dropout\n",
    "            gradiente = DnnLib.softmax_crossentropy_gradient(out_linear, y_batch)\n",
    "            backward_pass_with_dropout(capas, gradiente)\n",
    "            \n",
    "            # Actualizar capas (no dropout)\n",
    "            optimizers.update(capas[2])\n",
    "            optimizers.update(capas[0])\n",
    "\n",
    "            epoch_loss += perdida\n",
    "            n_batches += 1\n",
    "            preds = np.argmax(out_linear, axis=1)\n",
    "            labels = np.argmax(y_batch, axis=1)\n",
    "            correct += np.sum(preds == labels)\n",
    "            total += len(labels)\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            print(f\"Data Loss: {perdida:.4f}, \"\n",
    "                f\"Reg Loss: {total_reg_loss:.4f}, Total: {data_loss:.4f}\")\n",
    "            val_output = forward_pass_with_dropout(capas, X_val, training=False)\n",
    "            ps = val_output[-2]\n",
    "            out_linears = capas[-1].forward_linear(ps)\n",
    "            val_loss = DnnLib.cross_entropy(out_linears, y_val)\n",
    "            print(f\"Epoch {e}, Train Loss: {perdida:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {e}, Avg Loss: {avg_loss:.4f}, Acc: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1da4d7d8-c243-41e4-bd23-2e0b205445e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datos():\n",
    "    with open(\"mnist_entrenado.json\", \"r\") as f:\n",
    "        datos = json.load(f)\n",
    "\n",
    "    layer1 = DnnLib.DenseLayer(784, 128, DnnLib.ActivationType.RELU)  \n",
    "    dropout1 = DnnLib.Dropout(dropout_rate=0.5)\n",
    "    layer2 = DnnLib.DenseLayer(128, 10, DnnLib.ActivationType.SOFTMAX)\n",
    "    \n",
    "    layer1.weights = np.array(datos[\"layers\"][0][\"W\"], dtype=np.float32)\n",
    "    layer1.bias = np.array(datos[\"layers\"][0][\"b\"], dtype=np.float32)\n",
    "\n",
    "    layer2.weights = np.array(datos[\"layers\"][1][\"W\"], dtype=np.float32)\n",
    "    layer2.bias = np.array(datos[\"layers\"][1][\"b\"], dtype=np.float32)\n",
    "\n",
    "    print(layer1.weights.shape, layer1.bias.shape, layer2.weights.shape, layer2.bias.shape)\n",
    "\n",
    "    return [layer1, dropout1, layer2]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9618a419-0439-41c7-8ab4-bebcb3ab4693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 54000 samples\n",
      "Validation set: 6000 samples\n",
      "(128, 784) (128,) (10, 128) (10,)\n",
      "Epoch 1, Avg Loss: 7.4499, Acc: 46.81%\n",
      "Epoch 2, Avg Loss: 1.7017, Acc: 65.40%\n",
      "Epoch 3, Avg Loss: 1.1419, Acc: 70.57%\n",
      "Epoch 4, Avg Loss: 0.9096, Acc: 73.25%\n",
      "Epoch 5, Avg Loss: 0.7880, Acc: 75.18%\n",
      "Epoch 6, Avg Loss: 0.7316, Acc: 76.38%\n",
      "Epoch 7, Avg Loss: 0.7165, Acc: 77.36%\n",
      "Epoch 8, Avg Loss: 0.6885, Acc: 78.43%\n",
      "Epoch 9, Avg Loss: 0.6656, Acc: 79.08%\n",
      "Data Loss: 1.0838, Reg Loss: 1.9142, Total: 2.9980\n",
      "Epoch 10, Train Loss: 1.0838, Val Loss: 0.5341\n",
      "Epoch 10, Avg Loss: 0.6680, Acc: 79.70%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "imagenE = imagenes.reshape(-1, 784).astype(np.float32) / 255\n",
    "imagenP = imagens.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "val_ratio = 0.1\n",
    "n_train = imagenE.shape[0]\n",
    "n_val = int(n_train * val_ratio)\n",
    "\n",
    "indices = np.random.permutation(n_train)\n",
    "train_idx = indices[n_val:]  # índice para entrenamiento\n",
    "val_idx = indices[:n_val]    # índice para validación\n",
    "\n",
    "X_train, y_train_labels = imagenE[train_idx], labelE[train_idx]\n",
    "y_train = yE[train_idx]\n",
    "\n",
    "X_val, y_val_labels = imagenE[val_idx], labelE[val_idx]\n",
    "y_val = yE[val_idx]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "\n",
    "capas = load_datos()\n",
    "for L in capas:\n",
    "    if not hasattr(L, \"training\"):  \n",
    "        L.set_regularizer(DnnLib.RegularizerType.L2, 1e-4)\n",
    "\n",
    "optimizers = DnnLib.Adam(0.001)\n",
    "entrenamiento_reg(capas, optimizers, imagenE, yE, labelE, X_val, y_val, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1edfadbc-ba9b-4e70-8957-c36740799ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(128, 784) (128,) (10, 128) (10,)\n",
      "Predicción entrenamiento: 9.383333333333335\n",
      "Predicción test: 8.84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# carga datos de npz para labels e imagenes\n",
    "prueba = np.load(\"fashion_mnist_train.npz\")\n",
    "test = np.load(\"fashion_mnist_test.npz\")\n",
    "\n",
    "#separar en labels e imagenes\n",
    "imagenes = prueba[\"images\"]\n",
    "labelE = prueba[\"labels\"]\n",
    "\n",
    "imagens = test[\"images\"]\n",
    "labelP = test[\"labels\"]\n",
    "\n",
    "#reformar las imagenes(entradas)\n",
    "imagenE = imagenes.reshape(-1, 784).astype(np.float32) / 255\n",
    "imagenP = imagens.reshape(-1, 784).astype(np.float32) / 255\n",
    "print(imagenE.shape)\n",
    "\n",
    "#accuracy entre test y train de fashion mnist\n",
    "\n",
    "capas = load_datos()\n",
    "\n",
    "out = forward_pass_with_dropout(capas, imagenE)\n",
    "out1 = forward_pass_with_dropout(capas, imagenP)\n",
    "\n",
    "penultima = out[-2]\n",
    "out_linear = capas[-1].forward_linear(penultima)\n",
    "\n",
    "ps = out1[-2]\n",
    "ol = capas[-1].forward_linear(ps)\n",
    "\n",
    "# Para test\n",
    "predicts = np.argmax(out_linear, axis=1)\n",
    "acurr = np.mean(predicts == labelE)\n",
    "print(\"Predicción entrenamiento:\", acurr * 100)\n",
    "\n",
    "# Para entrenamiento\n",
    "predicts = np.argmax(ol, axis=1)\n",
    "acurr = np.mean(predicts == labelP)   \n",
    "print(\"Predicción test:\", acurr * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58543e12-cf97-45f5-9a2f-b15247b20abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Entrenamiento con dropout y regularización para Fashion-MNIST- Valeria\")\n",
    "\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50,\n",
    "                        help=\"Número de épocas para entrenar\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128,\n",
    "                        help=\"Tamaño de cada batch\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "\n",
    "    imagenE = imagenes.reshape(-1, 784).astype(np.float32) / 255\n",
    "    imagenP = imagens.reshape(-1, 784).astype(np.float32) / 255\n",
    "\n",
    "    val_ratio = 0.1\n",
    "    n_train = imagenE.shape[0]\n",
    "    n_val = int(n_train * val_ratio)\n",
    "\n",
    "    indices = np.random.permutation(n_train)\n",
    "    train_idx = indices[n_val:]  # índice para entrenamiento\n",
    "    val_idx = indices[:n_val]    # índice para validación\n",
    "\n",
    "    X_train, y_train_labels = imagenE[train_idx], labelE[train_idx]\n",
    "    y_train = yE[train_idx]\n",
    "\n",
    "    X_val, y_val_labels = imagenE[val_idx], labelE[val_idx]\n",
    "    y_val = yE[val_idx]\n",
    "\n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "    capas = load_datos_reg()\n",
    "    for L in capas:\n",
    "        if not hasattr(L, \"training\"):  # DenseLayer\n",
    "            L.set_regularizer(DnnLib.RegularizerType.L2, 1e-4)\n",
    "\n",
    "    optimizers = DnnLib.Adam(0.001)\n",
    "\n",
    "    entrenamiento_reg(\n",
    "        capas, optimizers,\n",
    "        imagenE, yE, labelE,\n",
    "        X_val, y_val,\n",
    "        epochs=args.epochs,\n",
    "        batches=args.batch_size\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
